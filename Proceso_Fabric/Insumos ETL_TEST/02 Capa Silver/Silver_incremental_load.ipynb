{"cells":[{"cell_type":"markdown","source":["#### Silver Layer - Incremental Load\n","- Purpose: Incrementally update Silver tables from Bronze layer changes\n","- Layer: Silver (Clean Data)\n","- Load Type: Incremental"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa120cef-aec3-4374-9876-b918ec0d57d4"},{"cell_type":"markdown","source":["---\n","### Dependencias\n","---"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de6cb15c-788f-4283-9c94-9354164c3565"},{"cell_type":"code","source":["import os\n","from datetime import datetime, timedelta\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from pyspark.sql.window import Window\n","import logging\n","from pyspark import StorageLevel"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"9cc54385-f449-4f4e-b6f3-310c3a7da6b1","normalized_state":"finished","queued_time":"2025-09-18T23:14:09.8318953Z","session_start_time":null,"execution_start_time":"2025-09-18T23:14:09.8330548Z","execution_finish_time":"2025-09-18T23:14:10.1515709Z","parent_msg_id":"e713bc27-bac6-4373-b509-cad5eb157d4b"},"text/plain":"StatementMeta(, 9cc54385-f449-4f4e-b6f3-310c3a7da6b1, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35799748-591a-49f4-80d4-bb6d470ef5bc"},{"cell_type":"markdown","source":["---\n","### Configuraciones de optimización\n","---"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"440fa288-a351-448f-8001-664f1857b39c"},{"cell_type":"code","source":["execution_date = os.environ.get(\"execution_date\", datetime.now().isoformat())\n","lookback_days = int(os.environ.get(\"lookback_days\", \"1\"))\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"9cc54385-f449-4f4e-b6f3-310c3a7da6b1","normalized_state":"finished","queued_time":"2025-09-18T23:14:09.9457714Z","session_start_time":null,"execution_start_time":"2025-09-18T23:14:10.1535915Z","execution_finish_time":"2025-09-18T23:14:10.4965891Z","parent_msg_id":"461d31c2-a8f3-42cd-b8a0-bd4bfcd2f898"},"text/plain":"StatementMeta(, 9cc54385-f449-4f4e-b6f3-310c3a7da6b1, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2df59b2f-3d62-4e77-963b-5623928602a2"},{"cell_type":"markdown","source":["---\n","### Configuraciones de optimización\n","---"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1ad3170b-0b6b-485c-8520-da6c16f78b8a"},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n","spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"9cc54385-f449-4f4e-b6f3-310c3a7da6b1","normalized_state":"finished","queued_time":"2025-09-18T23:14:10.0640988Z","session_start_time":null,"execution_start_time":"2025-09-18T23:14:10.4985402Z","execution_finish_time":"2025-09-18T23:14:10.8182961Z","parent_msg_id":"5628680b-b76f-4c0d-80f2-91e307e00bf8"},"text/plain":"StatementMeta(, 9cc54385-f449-4f4e-b6f3-310c3a7da6b1, 19, Finished, Available, Finished)"},"metadata":{}}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"df53d745-cf86-4c6a-aa90-6ed45016ad53"},{"cell_type":"markdown","source":["---\n","### Tratamiento de la data\n","---"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eafaaf98-7456-48a6-9924-c63af6bfb2ec"},{"cell_type":"code","source":["class SilverIncrementalProcessor:\n","    def __init__(self, spark_session):\n","        self.spark = spark_session\n","        \n","    def get_last_silver_execution_timestamp(self, table_name: str):\n","        \"\"\"Obtener la última marca de tiempo de ejecución exitosa para una tabla silver\"\"\"\n","        try:\n","            if not self.spark._jsparkSession.catalog().tableExists(\"silver_incremental_control\"):\n","                silver_table_name = table_name.replace('bronze_', 'silver_')\n","                if self.spark._jsparkSession.catalog().tableExists(silver_table_name):\n","                    max_date = self.spark.table(silver_table_name).agg(max(\"silver_created_date\")).collect()[0][0]\n","                    logger.info(f\"Usando max silver_created_date para {table_name}: {max_date}\")\n","                    return max_date\n","                return None\n","                \n","            control_df = self.spark.table(\"silver_incremental_control\")\n","            last_run = control_df.filter(\n","                (col(\"table_name\") == table_name) & \n","                (col(\"status\") == \"success\")\n","            ).orderBy(col(\"execution_timestamp\").desc()).limit(1).collect()\n","            \n","            if last_run:\n","                return last_run[0][\"last_processed_timestamp\"]\n","            else:\n","                silver_table_name = table_name.replace('bronze_', 'silver_')\n","                if self.spark._jsparkSession.catalog().tableExists(silver_table_name):\n","                    max_date = self.spark.table(silver_table_name).agg(max(\"silver_created_date\")).collect()[0][0]\n","                    logger.info(f\"Primera ejecución incremental para {table_name}. Usando max date: {max_date}\")\n","                    return max_date\n","                return None\n","        except Exception as e:\n","            logger.warning(f\"No se pudo obtener la marca de tiempo para {table_name}: {e}\")\n","            return None\n","    \n","    def update_silver_execution_control(self, table_name: str, last_processed_timestamp, status: str, record_count: int = 0):\n","        \"\"\"Actualizar la tabla de control silver con la información de ejecución\"\"\"\n","        control_data = [(\n","            table_name,\n","            execution_date,\n","            datetime.now(),\n","            last_processed_timestamp,\n","            status,\n","            record_count\n","        )]\n","        \n","        control_schema = StructType([\n","            StructField(\"table_name\", StringType(), True),\n","            StructField(\"execution_id\", StringType(), True),\n","            StructField(\"execution_timestamp\", TimestampType(), True),\n","            StructField(\"last_processed_timestamp\", TimestampType(), True),\n","            StructField(\"status\", StringType(), True),\n","            StructField(\"record_count\", IntegerType(), True)\n","        ])\n","        \n","        control_df = self.spark.createDataFrame(control_data, control_schema)\n","        control_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"silver_incremental_control\")\n","    \n","    def get_bronze_tables(self):\n","        \"\"\"Obtener la lista de tablas bronze a procesar\"\"\"\n","        tables_df = self.spark.sql(\"\"\"\n","            SHOW TABLES LIKE 'bronze_*'\n","        \"\"\")\n","        all_tables = [row.tableName for row in tables_df.collect()]\n","        \n","        # Excluir tablas de control y metadata\n","        excluded_tables = ['bronze_execution_log', 'bronze_notebook_execution_summary']\n","        filtered_tables = [table for table in all_tables if table not in excluded_tables]\n","        \n","        logger.info(f\"Found {len(filtered_tables)} business tables to process (excluded {len(excluded_tables)} metadata tables)\")\n","        return filtered_tables\n","    \n","    def standardize_data_types(self, df, table_name):\n","        schema_dict = dict(df.dtypes)\n","        transformations = []\n","        \n","        for col_name, col_type in schema_dict.items():\n","            new_col = col_name\n","            \n","            if col_name.lower().endswith('date') and col_type == 'bigint':\n","                new_col = when(\n","                    col(col_name) > 1000000000000,  # Si > año 2001, son nanosegundos\n","                    from_unixtime(col(col_name) / 1000000000).cast(TimestampType())\n","                ).otherwise(\n","                    from_unixtime(col(col_name)).cast(TimestampType())\n","                ).alias(col_name)\n","                transformations.append(new_col)\n","            \n","            elif col_name.lower() == 'month' and col_type == 'bigint':\n","                new_col = from_unixtime(col(col_name) / 1000000000).cast(TimestampType()).alias(col_name)\n","                transformations.append(new_col)\n","                \n","            elif col_name.lower() == 'dwcreateddate' and col_type != 'timestamp':\n","                if col_type in ['string', 'datetime']:\n","                    new_col = to_timestamp(col(col_name)).alias(col_name)\n","                    transformations.append(new_col)\n","                else:\n","                    transformations.append(col(col_name))\n","            \n","            elif col_name.lower().endswith(('_key', 'key')) and col_type == 'string':\n","                new_col = when(\n","                    trim(upper(col(col_name))).isin(\"\", \"NULL\", \"N/A\", \"UNKNOWN\", \"NONE\") |\n","                    trim(col(col_name)).isNull(),\n","                    None\n","                ).otherwise(\n","                    trim(upper(col(col_name))) \n","                ).alias(col_name)\n","                transformations.append(new_col)\n","            \n","            elif col_type == 'string':\n","                new_col = when(\n","                    trim(upper(col(col_name))).isin(\"\", \"NULL\", \"N/A\", \"UNKNOWN\", \"NONE\", \"#N/A\") |\n","                    trim(col(col_name)).isNull(),\n","                    None\n","                ).otherwise(\n","                    trim(col(col_name))\n","                ).alias(col_name)\n","                transformations.append(new_col)\n","            \n","            elif col_type in ['double', 'float']:\n","                new_col = when(\n","                    isnan(col(col_name)) | col(col_name).isNull(), None\n","                ).otherwise(col(col_name)).alias(col_name)\n","                transformations.append(new_col)\n","            \n","            elif col_type == 'boolean':\n","                transformations.append(col(col_name))\n","                \n","            else:\n","                transformations.append(col(col_name))\n","        \n","        if transformations:\n","            df = df.select(*transformations)\n","            \n","        return df\n","    \n","    def create_quarantine_records(self, df, table_name):\n","        \"\"\"NUEVO: Crear registros de cuarentena para datos problemáticos\"\"\"                \n","        problem_filter = None\n","        \n","        # Filtro para fechas futuras irreales\n","        future_cutoff = date_add(current_date(), 730)  # 2 años en el futuro\n","        for col_name in df.columns:\n","            if col_name.lower().endswith('date') and col_name.lower() != 'dwcreateddate':\n","                date_condition = col(col_name) > future_cutoff\n","                problem_filter = date_condition if problem_filter is None else (problem_filter | date_condition)\n","        \n","        for col_name, col_type in dict(df.dtypes).items():\n","            if col_type in ['double', 'float'] and 'value' in col_name.lower():\n","                value_condition = (col(col_name) > 100000000) | (col(col_name) < -10000000)\n","                problem_filter = value_condition if problem_filter is None else (problem_filter | value_condition)\n","        \n","        if problem_filter is not None:\n","            try:\n","                quarantine_df = df.filter(problem_filter)\n","                clean_df = df.filter(~problem_filter)\n","                \n","                if quarantine_df.count() > 0:\n","                    quarantine_table = f\"silver_quarantine_{table_name.replace('silver_', '')}\"\n","                    \n","                    quarantine_with_metadata = (\n","                        quarantine_df\n","                        .withColumn(\"quarantine_date\", current_timestamp())\n","                        .withColumn(\"quarantine_reason\", lit(\"data_quality_issues\"))\n","                        .withColumn(\"source_table\", lit(table_name))\n","                    )\n","                    \n","                    quarantine_with_metadata.write.mode(\"append\").format(\"delta\").saveAsTable(quarantine_table)\n","                    logger.info(f\"Quarantined {quarantine_df.count()} problematic records from {table_name}\")\n","                \n","                return clean_df\n","            except Exception as e:\n","                logger.warning(f\"Quarantine process failed for {table_name}: {str(e)}. Returning original data.\")\n","                return df\n","        return df\n","    \n","    def optimize_partitioning(self, df, table_name):\n","        \"\"\"NUEVO: Optimización dinámica de particiones\"\"\"\n","        row_count = df.count()\n","        current_partitions = df.rdd.getNumPartitions()\n","        \n","        if row_count > 1000000: \n","            base_partitions = row_count // 150000\n","            if base_partitions < 4:\n","                optimal_partitions = 4\n","            elif base_partitions > 200:\n","                optimal_partitions = 200\n","            else:\n","                optimal_partitions = base_partitions\n","        else:  \n","            calculated_partitions = row_count // 50000\n","            optimal_partitions = 1 if calculated_partitions < 1 else calculated_partitions\n","        \n","        if current_partitions != optimal_partitions:\n","            logger.info(f\"{table_name}: Repartitioning from {current_partitions} to {optimal_partitions}\")\n","            df = df.repartition(optimal_partitions)\n","        return df\n","    \n","    def remove_duplicates(self, df, table_name):\n","        \"\"\"Eliminar duplicados conservando el registro más reciente - MEJORADO\"\"\"\n","        if table_name.startswith('silver_dim_'):\n","            cols_except_created = [col for col in df.columns if col.lower() not in ['dwcreateddate', 'silver_created_date', 'silver_execution_id']]\n","            \n","            if 'dwcreateddate' in df.columns and cols_except_created:\n","                window_spec = Window.partitionBy(*cols_except_created).orderBy(desc('dwcreateddate'))\n","                df = df.withColumn('rn', row_number().over(window_spec)) \\\n","                      .filter(col('rn') == 1) \\\n","                      .drop('rn')\n","                      \n","        elif table_name.startswith('silver_fact_'):\n","            key_columns = []\n","            for col_name in df.columns:\n","                if any(pattern in col_name.lower() for pattern in ['_number', '_key', 'customer_key', 'product_key']):\n","                    key_columns.append(col_name)\n","            \n","            if key_columns and 'dwcreateddate' in df.columns:\n","                window_spec = Window.partitionBy(*key_columns).orderBy(desc('dwcreateddate'))\n","                df = df.withColumn('rn', row_number().over(window_spec)) \\\n","                      .filter(col('rn') == 1) \\\n","                      .drop('rn')\n","        \n","        return df\n","    \n","    def data_quality_checks(self, df, table_name):\n","        \"\"\"Chequeos básicos de calidad de datos y limpieza - MEJORADO\"\"\"\n","        initial_count = df.count()\n","        df_clean = df.dropna(how='all')\n","        \n","        # Aplicar cuarentena\n","        df_clean = self.create_quarantine_records(df_clean, table_name)\n","        \n","        final_count = df_clean.count()\n","        processed_rows = initial_count - final_count\n","        \n","        if processed_rows > 0 and initial_count > 0:\n","            logger.info(f\"{table_name}: Processed {processed_rows} problematic rows ({(processed_rows/initial_count)*100:.2f}%)\")\n","        \n","        return df_clean\n","    \n","    def get_incremental_bronze_data(self, bronze_table_name, last_processed_timestamp):\n","        \"\"\"Obtener datos incrementales de la tabla bronze\"\"\"\n","        bronze_df = self.spark.table(bronze_table_name)\n","        \n","        if last_processed_timestamp is None:\n","            logger.info(f\"Primera carga incremental para {bronze_table_name} - procesando todos los datos\")\n","            return bronze_df\n","        \n","        safe_timestamp = last_processed_timestamp - timedelta(hours=1)\n","        logger.info(f\"Carga incremental para {bronze_table_name} desde {safe_timestamp}\")\n","        \n","        if 'dwcreateddate' in bronze_df.columns:\n","            incremental_df = bronze_df.filter(col('dwcreateddate') > safe_timestamp)\n","        else:\n","            logger.warning(f\"No existe la columna dwcreateddate en {bronze_table_name}, procesando todos los datos\")\n","            incremental_df = bronze_df\n","            \n","        return incremental_df\n","    \n","    def merge_to_silver_table(self, new_df, silver_table_name):\n","        \"\"\"Fusionar datos incrementales en la tabla silver destino - MEJORADO\"\"\"\n","        if not self.spark._jsparkSession.catalog().tableExists(silver_table_name):\n","            logger.info(f\"Creando nueva tabla silver: {silver_table_name}\")\n","            record_count = new_df.count()\n","            \n","            writer = new_df.write.mode(\"overwrite\").format(\"delta\")\n","            \n","            if 'fact' in silver_table_name:\n","                writer = (writer\n","                          .option(\"optimizeWrite\", \"true\")\n","                          .option(\"autoCompact\", \"true\")\n","                          .option(\"dataSkippingNumIndexedCols\", \"5\"))\n","            else:\n","                writer = writer.option(\"optimizeWrite\", \"true\")\n","            \n","            writer.saveAsTable(silver_table_name)\n","            logger.info(f\"Final partitions for {silver_table_name}: {new_df.rdd.getNumPartitions()}\")\n","            return record_count\n","    \n","        logger.info(f\"Fusionando datos en la tabla silver existente: {silver_table_name}\")\n","        existing_df = self.spark.table(silver_table_name)\n","        combined_df = existing_df.union(new_df)\n","        \n","        if silver_table_name.startswith('silver_dim_'):\n","            cols_except_meta = [col for col in combined_df.columns \n","                               if col.lower() not in ['dwcreateddate', 'silver_created_date', 'silver_execution_id']]\n","            \n","            if 'dwcreateddate' in combined_df.columns and cols_except_meta:\n","                window_spec = Window.partitionBy(*cols_except_meta).orderBy(desc('dwcreateddate'))\n","                final_df = combined_df.withColumn('rn', row_number().over(window_spec)) \\\n","                                     .filter(col('rn') == 1) \\\n","                                     .drop('rn')\n","            else:\n","                final_df = combined_df\n","                \n","        elif silver_table_name.startswith('silver_fact_'):\n","            key_columns = []\n","            for col_name in combined_df.columns:\n","                if any(pattern in col_name.lower() for pattern in ['_number', '_key', 'customer_key', 'product_key']):\n","                    key_columns.append(col_name)\n","            \n","            if key_columns and 'dwcreateddate' in combined_df.columns:\n","                window_spec = Window.partitionBy(*key_columns).orderBy(desc('dwcreateddate'))\n","                final_df = combined_df.withColumn('rn', row_number().over(window_spec)) \\\n","                                     .filter(col('rn') == 1) \\\n","                                     .drop('rn')\n","            else:\n","                final_df = combined_df\n","        else:\n","            final_df = combined_df\n","        \n","        record_count = final_df.count()\n","        \n","        writer = final_df.write.mode(\"overwrite\").format(\"delta\")\n","        \n","        if 'fact' in silver_table_name:\n","            writer = (writer\n","                      .option(\"optimizeWrite\", \"true\")\n","                      .option(\"autoCompact\", \"true\")\n","                      .option(\"dataSkippingNumIndexedCols\", \"5\"))\n","        else:\n","            writer = writer.option(\"optimizeWrite\", \"true\")\n","        \n","        writer.saveAsTable(silver_table_name)\n","        logger.info(f\"Final partitions for {silver_table_name}: {final_df.rdd.getNumPartitions()}\")\n","        \n","        return record_count\n","    \n","    def process_silver_incremental(self, bronze_table_name):\n","        \"\"\"Procesar actualizaciones incrementales para una tabla bronze - MEJORADO\"\"\"\n","        try:\n","            silver_table_name = bronze_table_name.replace('bronze_', 'silver_')\n","            last_timestamp = self.get_last_silver_execution_timestamp(bronze_table_name)\n","            logger.info(f\"Procesando {bronze_table_name} -> {silver_table_name}\")\n","            bronze_incremental = self.get_incremental_bronze_data(bronze_table_name, last_timestamp)\n","            \n","            if bronze_incremental.count() == 0:\n","                logger.info(f\"No hay nuevos datos para {bronze_table_name}\")\n","                self.update_silver_execution_control(bronze_table_name, last_timestamp or datetime.now(), \"success\", 0)\n","                return 0\n","            \n","            df_processed = bronze_incremental\n","            df_processed = self.standardize_data_types(df_processed, silver_table_name)\n","            df_processed = self.remove_duplicates(df_processed, silver_table_name)\n","            df_processed = self.data_quality_checks(df_processed, silver_table_name)\n","            df_processed = self.optimize_partitioning(df_processed, silver_table_name)  # NUEVO\n","            \n","            df_processed = df_processed.withColumn('silver_created_date', current_timestamp()) \\\n","                                       .withColumn('silver_execution_id', lit(execution_date))\n","\n","            df_processed = df_processed.persist(StorageLevel.MEMORY_AND_DISK)\n","            \n","            max_timestamp = None\n","            if 'dwcreateddate' in df_processed.columns:\n","                max_timestamp = df_processed.agg(max('dwcreateddate')).collect()[0][0]\n","            if max_timestamp is None:\n","                max_timestamp = datetime.now()\n","            \n","            final_record_count = self.merge_to_silver_table(df_processed, silver_table_name)\n","            self.update_silver_execution_control(bronze_table_name, max_timestamp, \"success\", df_processed.count())\n","            df_processed.unpersist()\n","            \n","            logger.info(f\"Procesado exitosamente {silver_table_name}: {df_processed.count():,} nuevos registros, {final_record_count:,} en total\")\n","            return df_processed.count()\n","            \n","        except Exception as e:\n","            logger.error(f\"Error procesando {bronze_table_name}: {str(e)}\")\n","            self.update_silver_execution_control(bronze_table_name, None, \"failed\", 0)\n","            raise e"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"9cc54385-f449-4f4e-b6f3-310c3a7da6b1","normalized_state":"finished","queued_time":"2025-09-18T23:14:10.329041Z","session_start_time":null,"execution_start_time":"2025-09-18T23:14:10.8209178Z","execution_finish_time":"2025-09-18T23:14:11.3176318Z","parent_msg_id":"16adb58e-4eca-42c1-acee-a2428c454c63"},"text/plain":"StatementMeta(, 9cc54385-f449-4f4e-b6f3-310c3a7da6b1, 20, Finished, Available, Finished)"},"metadata":{}}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc7262db-0984-4a76-92d8-d445a3e6e88e"},{"cell_type":"code","source":["silver_processor = SilverIncrementalProcessor(spark)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"9cc54385-f449-4f4e-b6f3-310c3a7da6b1","normalized_state":"finished","queued_time":"2025-09-18T23:14:10.7008442Z","session_start_time":null,"execution_start_time":"2025-09-18T23:14:11.3195861Z","execution_finish_time":"2025-09-18T23:14:11.7256721Z","parent_msg_id":"fb490824-c685-4d00-9c2b-184886bad9bd"},"text/plain":"StatementMeta(, 9cc54385-f449-4f4e-b6f3-310c3a7da6b1, 21, Finished, Available, Finished)"},"metadata":{}}],"execution_count":19,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c7bdd2e5-f3e5-4d3c-ba09-a933358a6cc4"},{"cell_type":"code","source":["bronze_tables = silver_processor.get_bronze_tables()\n","logger.info(f\"Found {len(bronze_tables)} bronze tables for incremental processing\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"9cc54385-f449-4f4e-b6f3-310c3a7da6b1","normalized_state":"finished","queued_time":"2025-09-18T23:14:10.9835387Z","session_start_time":null,"execution_start_time":"2025-09-18T23:14:11.7275203Z","execution_finish_time":"2025-09-18T23:14:12.6987818Z","parent_msg_id":"44a0a8dd-82dc-41f5-819f-292b0ef27023"},"text/plain":"StatementMeta(, 9cc54385-f449-4f4e-b6f3-310c3a7da6b1, 22, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:Found 14 bronze tables for incremental processing\n"]}],"execution_count":20,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4757ad69-3f5c-4157-90c4-19c6ebc893b4"},{"cell_type":"code","source":["results = []\n","total_records = 0\n","\n","for bronze_table in bronze_tables:\n","    try:\n","        record_count = silver_processor.process_silver_incremental(bronze_table)\n","        silver_table = bronze_table.replace('bronze_', 'silver_')\n","        \n","        results.append({\n","            'bronze_table': bronze_table,\n","            'silver_table': silver_table,\n","            'record_count': record_count,\n","            'status': 'success'\n","        })\n","        total_records += record_count\n","        print(f\"✓ {silver_table}: {record_count:,} records processed\")\n","        \n","    except Exception as e:\n","        results.append({\n","            'bronze_table': bronze_table,\n","            'silver_table': bronze_table.replace('bronze_', 'silver_'),\n","            'record_count': 0,\n","            'status': 'failed',\n","            'error': str(e)\n","        })\n","        print(f\"✗ {bronze_table}: FAILED - {str(e)}\")\n","    \n","    print(\"-\" * 50)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","session_id":"9cc54385-f449-4f4e-b6f3-310c3a7da6b1","normalized_state":"finished","queued_time":"2025-09-18T23:14:11.2761552Z","session_start_time":null,"execution_start_time":"2025-09-18T23:14:12.7008479Z","execution_finish_time":"2025-09-18T23:15:30.5316991Z","parent_msg_id":"bf3e3010-eec2-4e6f-b4ac-0f7f1110edbf"},"text/plain":"StatementMeta(, 9cc54385-f449-4f4e-b6f3-310c3a7da6b1, 23, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:Procesando bronze_dim_brands -> silver_dim_brands\nINFO:__main__:Carga incremental para bronze_dim_brands desde 2025-09-18 21:17:48.136527\nINFO:__main__:No hay nuevos datos para bronze_dim_brands\nINFO:__main__:Procesando bronze_dim_budget_rate -> silver_dim_budget_rate\nINFO:__main__:Carga incremental para bronze_dim_budget_rate desde 2025-09-18 21:17:56.290482\nINFO:__main__:No hay nuevos datos para bronze_dim_budget_rate\nINFO:__main__:Procesando bronze_dim_customers -> silver_dim_customers\nINFO:__main__:Carga incremental para bronze_dim_customers desde 2025-09-18 21:18:04.726647\nINFO:__main__:No hay nuevos datos para bronze_dim_customers\nINFO:__main__:Procesando bronze_dim_employees -> silver_dim_employees\nINFO:__main__:Carga incremental para bronze_dim_employees desde 2025-09-18 21:18:14.040441\nINFO:__main__:No hay nuevos datos para bronze_dim_employees\nINFO:__main__:Procesando bronze_dim_exchange_rate -> silver_dim_exchange_rate\nINFO:__main__:Carga incremental para bronze_dim_exchange_rate desde 2025-09-18 21:18:22.393722\nINFO:__main__:No hay nuevos datos para bronze_dim_exchange_rate\nINFO:__main__:Procesando bronze_dim_invoice_doctype -> silver_dim_invoice_doctype\nINFO:__main__:Carga incremental para bronze_dim_invoice_doctype desde 2025-09-18 21:18:30.962186\nINFO:__main__:No hay nuevos datos para bronze_dim_invoice_doctype\nINFO:__main__:Procesando bronze_dim_order_doctype -> silver_dim_order_doctype\nINFO:__main__:Carga incremental para bronze_dim_order_doctype desde 2025-09-18 21:18:37.632472\nINFO:__main__:No hay nuevos datos para bronze_dim_order_doctype\nINFO:__main__:Procesando bronze_dim_order_status -> silver_dim_order_status\nINFO:__main__:Carga incremental para bronze_dim_order_status desde 2025-09-18 21:18:44.004694\nINFO:__main__:No hay nuevos datos para bronze_dim_order_status\nINFO:__main__:Procesando bronze_dim_products -> silver_dim_products\nINFO:__main__:Carga incremental para bronze_dim_products desde 2025-09-18 21:18:56.001700\nINFO:__main__:No hay nuevos datos para bronze_dim_products\nINFO:__main__:Procesando bronze_dim_regions -> silver_dim_regions\nINFO:__main__:Carga incremental para bronze_dim_regions desde 2025-09-18 21:19:07.926886\nINFO:__main__:No hay nuevos datos para bronze_dim_regions\nINFO:__main__:Procesando bronze_fact_budget -> silver_fact_budget\nINFO:__main__:Carga incremental para bronze_fact_budget desde 2025-09-18 21:19:35.110934\nINFO:__main__:No hay nuevos datos para bronze_fact_budget\nINFO:__main__:Procesando bronze_fact_forecast -> silver_fact_forecast\nINFO:__main__:Carga incremental para bronze_fact_forecast desde 2025-09-18 21:19:52.839272\nINFO:__main__:No hay nuevos datos para bronze_fact_forecast\nINFO:__main__:Procesando bronze_fact_invoices -> silver_fact_invoices\nINFO:__main__:Carga incremental para bronze_fact_invoices desde 2025-09-18 21:22:09.055227\nINFO:__main__:No hay nuevos datos para bronze_fact_invoices\nINFO:__main__:Procesando bronze_fact_orders -> silver_fact_orders\nINFO:__main__:Carga incremental para bronze_fact_orders desde 2025-09-18 21:28:10.389906\nINFO:__main__:No hay nuevos datos para bronze_fact_orders\n"]},{"output_type":"stream","name":"stdout","text":["✓ silver_dim_brands: 0 records processed\n--------------------------------------------------\n✓ silver_dim_budget_rate: 0 records processed\n--------------------------------------------------\n✓ silver_dim_employees: 0 records processed\n--------------------------------------------------\n✓ silver_dim_exchange_rate: 0 records processed\n--------------------------------------------------\n✓ silver_dim_invoice_doctype: 0 records processed\n--------------------------------------------------\n✓ silver_dim_order_status: 0 records processed\n--------------------------------------------------\n✓ silver_dim_products: 0 records processed\n--------------------------------------------------\n✓ silver_dim_regions: 0 records processed\n--------------------------------------------------\n✓ silver_fact_budget: 0 records processed\n--------------------------------------------------\n✓ silver_fact_orders: 0 records processed\n--------------------------------------------------\n"]}],"execution_count":21,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9bf2e285-c7cc-48f2-8dd3-0d3f0bbe6b75"},{"cell_type":"markdown","source":["---\n","### Logs del proceso\n","---"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ae8db43-946e-4f35-bae0-3de63d4b97f9"},{"cell_type":"code","source":["execution_log_data = [(\n","    execution_date,\n","    \"silver_incremental_load\",\n","    datetime.now(),\n","    \"completed\" if all(r[\"status\"] == \"success\" for r in results) else \"completed_with_errors\",\n","    \"silver\",\n","    \"incremental\",\n","    total_records,\n","    len([r for r in results if r[\"status\"] == \"success\"]),\n","    len([r for r in results if r[\"status\"] == \"failed\"]),\n","    len([r for r in results if r.get(\"record_count\", 0) > 0]),  # Tables with new data\n","    str(results)[:1000]\n",")]\n","\n","execution_log_schema = StructType([\n","    StructField(\"execution_id\", StringType(), True),\n","    StructField(\"pipeline_name\", StringType(), True),\n","    StructField(\"execution_timestamp\", TimestampType(), True),\n","    StructField(\"status\", StringType(), True),\n","    StructField(\"layer\", StringType(), True),\n","    StructField(\"load_type\", StringType(), True),\n","    StructField(\"total_records\", LongType(), True),\n","    StructField(\"successful_tables\", IntegerType(), True),\n","    StructField(\"failed_tables\", IntegerType(), True),\n","    StructField(\"updated_tables\", IntegerType(), True),\n","    StructField(\"details\", StringType(), True)\n","])\n","\n","execution_log = spark.createDataFrame(execution_log_data, execution_log_schema)\n","execution_log.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"silver_execution_log\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":24,"statement_ids":[24],"state":"finished","livy_statement_state":"available","session_id":"9cc54385-f449-4f4e-b6f3-310c3a7da6b1","normalized_state":"finished","queued_time":"2025-09-18T23:14:11.4352961Z","session_start_time":null,"execution_start_time":"2025-09-18T23:15:30.5340781Z","execution_finish_time":"2025-09-18T23:15:34.3260601Z","parent_msg_id":"aa67635a-5b23-4ef5-87cf-10ea43e740ff"},"text/plain":"StatementMeta(, 9cc54385-f449-4f4e-b6f3-310c3a7da6b1, 24, Finished, Available, Finished)"},"metadata":{}}],"execution_count":22,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"873c3749-8846-4fb2-96f8-ca673db305f4"},{"cell_type":"markdown","source":["---\n","### Resumen del proceso\n","---"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e430126b-46eb-48d3-919f-1b9079b83466"},{"cell_type":"code","source":["successful_loads = len([r for r in results if r[\"status\"] == \"success\"])\n","failed_loads = len([r for r in results if r[\"status\"] == \"failed\"])\n","updated_tables = len([r for r in results if r.get(\"record_count\", 0) > 0])\n","\n","print(\"=\" * 60)\n","print(\"SILVER INCREMENTAL LOAD SUMMARY:\")\n","print(f\"Successful processes: {successful_loads}\")\n","print(f\"Failed processes: {failed_loads}\")\n","print(f\"Tables with updates: {updated_tables}\")\n","print(f\"Total records processed: {total_records:,}\")\n","print(f\"Execution Date: {execution_date}\")\n","print(\"=\" * 60)\n","\n","if updated_tables > 0:\n","    print(\"\\nTables with incremental updates:\")\n","    for result in results:\n","        if result[\"status\"] == \"success\" and result[\"record_count\"] > 0:\n","            print(f\"  - {result['silver_table']}: {result['record_count']:,} records\")\n","\n","for result in results:\n","    if result[\"status\"] == \"success\" and result[\"record_count\"] > 0:\n","        try:\n","            spark.sql(f\"OPTIMIZE {result['silver_table']}\")\n","        except Exception as e:\n","            print(f\"✗ Error optimizing {result['silver_table']}: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":25,"statement_ids":[25],"state":"finished","livy_statement_state":"available","session_id":"9cc54385-f449-4f4e-b6f3-310c3a7da6b1","normalized_state":"finished","queued_time":"2025-09-18T23:14:11.5573736Z","session_start_time":null,"execution_start_time":"2025-09-18T23:15:34.3282839Z","execution_finish_time":"2025-09-18T23:15:34.7186709Z","parent_msg_id":"56598c9b-c369-42cd-a2d0-9c0500b8a8c6"},"text/plain":"StatementMeta(, 9cc54385-f449-4f4e-b6f3-310c3a7da6b1, 25, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["============================================================\nSILVER INCREMENTAL LOAD SUMMARY:\nSuccessful processes: 14\nFailed processes: 0\nTables with updates: 0\nTotal records processed: 0\nExecution Date: 2025-09-18T23:14:10.292146\n============================================================\n"]}],"execution_count":23,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f920a453-6c6d-4413-8ce3-63732c848eb0"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"es"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"a934a70f-0792-489f-ad5a-1c636112596a"}],"default_lakehouse":"a934a70f-0792-489f-ad5a-1c636112596a","default_lakehouse_name":"spaceparts_fabric_lh","default_lakehouse_workspace_id":"46a71285-4d57-4c8d-9bf6-2b3c7d697c2a"}}},"nbformat":4,"nbformat_minor":5}